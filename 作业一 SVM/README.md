# 2024Fall机器学习与数据挖掘 ——作业一

学号：22336084 

姓名：胡舸耀

## 一、实验环境

python：3.10.13

编译器：vscode

## 二、实验要求

1) 考虑两种不同的核函数：i) 线性核函数; ii) 高斯核函数
2) 可以直接调用现成 SVM 软件包来实现
3) 手动实现采用 hinge loss 和 cross-entropy loss 的线性分类模型，并比较它们的优劣

## 三、实验原理

支持向量机（Support vector Machine，SVM）是一种经典的监督学习算法，用于解决二分类和多分类问题。其核心思想是通过在特征空间中找到一个最优的超平面来进行分类，并且间隔最大。

在这个问题中，我们可以看作用一条线（超平面）来将数据分为两类，因为数据不一定可以用单条直线将数据分为两类，我们就可以利用核函数，将平面的输入数据转换到特征空间，再利用超平面将数据分为两类，并且间隔最大。

在二维空间中有一直线为:

$$
y=ax+b
$$

令$x$为$x_1$，$y$为$x_2$，移项得：

$$
ax_1-x_2+b=0
$$

向量化为：

$$
\omega^TX+\gamma=0
$$

$$
\omega=[\omega_1,\omega_2]^T \quad X=[x_1,x_2]^T
$$

在这里$\omega_1=a\quad \omega_2=-1$，此时$\omega$和原来$a$所代表的斜率垂直，即为直线的法向量，进一步拓展到多维中，此时：

$$
\omega=[\omega_1,\omega_2,···,\omega_n]^T \quad X=[x_1,x_2,···,x_n]^T
$$

这就是我们要求超平面的方程

为了确定$$\omega$$中的参数，我们要求出最大间隔时的参数值，参考直线中点到线的距离，间隔$d$有：

$$
d=\frac {|\omega^Tx+\gamma|}{||\omega||}
$$

我们的目的是为了找出一个分类效果好的超平面作为分类器。分类器的好坏的评定依据是分类间隔$W=2d$的大小，即分类间隔$W$越大，我们认为这个超平面的分类效果越好。此时，求解超平面的问题就变成了求解分类间隔$W$最大化的为题。W的最大化也就是$d$最大化的。所以SVM就是要找出最大的$d$时，各个参数的值。
